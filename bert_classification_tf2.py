# -*- coding: utf-8 -*-
"""Bert_classification_tf2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-VEAuzEoPOz9PhAeAcls-C7OiiR_vxx_
"""

from tensorflow.keras import layers,initializers
from transformers import modeling_tf_utils

from transformers import BertTokenizer,TFBertModel,TFBertForSequenceClassification,modeling_tf_utils
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers

class BERT_Classification():
  def __init__(self,x_train, y_train, x_val, y_val, nums_category, batch_size):
    self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
    self.batch_size = batch_size
    print('Start Tokenization')
    self.input_ids_train, self.attention_masks_train, self.labels_train = self.tokenization(x_train,y_train,self.tokenizer)
    self.input_ids_val, self.attention_masks_val, self.labels_val = self.tokenization(x_val,y_val,self.tokenizer)
    self.nums_category = nums_category
  
  def tokenization(self, sentences,labels,tokenizer):
    #input_id : Indices of input sequence tokens in the vocabulary.
    #attention_mask: Mask to avoid performing attention on padding token indices
    input_ids = []
    attention_masks = []

    # For every sentence...
    for sent in sentences:
        # `encode_plus` will:
        #   (1) Tokenize the sentence.
        #   (2) Prepend the `[CLS]` token to the start.
        #   (3) Append the `[SEP]` token to the end.
        #   (4) Map tokens to their IDs.
        #   (5) Pad or truncate the sentence to `max_length`
        #   (6) Create attention masks for [PAD] tokens.
        encoded_dict = self.tokenizer.encode_plus(
                            sent,                      # Sentence to encode.
                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'
                            max_length = 512,           # Pad & truncate all sentences.
                            pad_to_max_length = True,
                            return_attention_mask = True,   # Construct attn. masks.
                            return_tensors = 'tf',     # Return pytorch tensors.
                      )
        
        # Add the encoded sentence to the list.    
        input_ids.append(encoded_dict['input_ids'])
        
        # And its attention mask (simply differentiates padding from non-padding).
        attention_masks.append(encoded_dict['attention_mask'])

    # Convert the lists into tensors.
    input_ids = np.concatenate(input_ids, axis=0)
    attention_masks = np.concatenate(attention_masks, axis=0)
    labels = np.array(labels)
    return input_ids, attention_masks, labels
  
  def create_model_sequence_output(self, trainable=True):
    ## BERT encoder
    encoder = TFBertModel.from_pretrained("bert-base-uncased")
    encoder.trainable = trainable

    input_ids = layers.Input(shape=(512,), dtype=tf.int32)
    attention_mask = layers.Input(shape=(512,), dtype=tf.int32)

    sequence_output, pooled_output = encoder(
        input_ids, attention_mask=attention_mask
    )
    averaged = tf.reduce_mean(sequence_output, axis=1)
    dropout = layers.Dropout(0.1)(averaged)
    out = layers.Dense(self.nums_category, kernel_initializer=modeling_tf_utils.get_initializer(0.02))(dropout)
    
    model = tf.keras.Model(
        inputs=[input_ids, attention_mask],
        outputs=[out],
    )
    
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=["acc"])
    model.summary()
    return model

  def create_model_cls_output(self, trainable=True):
    ## BERT encoder
    encoder = TFBertModel.from_pretrained("bert-base-uncased")
    encoder.trainable = trainable

    input_ids = layers.Input(shape=(512,), dtype=tf.int32)
    attention_mask = layers.Input(shape=(512,), dtype=tf.int32)

    sequence_output, pooled_output = encoder(
        input_ids, attention_mask=attention_mask
    )
    #averaged = tf.reduce_mean(sequence_output, axis=1)
    dropout = layers.Dropout(0.1)(pooled_output)
    out = layers.Dense(self.nums_category, kernel_initializer=modeling_tf_utils.get_initializer(0.02))(dropout)
    
    model = tf.keras.Model(
        inputs=[input_ids, attention_mask],
        outputs=[out],
    )
    
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=["acc"])
    model.summary()
    return model
  
  def create_model_2(self):
    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=["acc"])
    model.summary()
    return model
  
  def train(self, model,epochs):
    model.fit(
      [self.input_ids_train, self.attention_masks_train],
      self.labels_train,
      epochs=epochs,  
      batch_size=self.batch_size,
      validation_data=([self.input_ids_val, self.attention_masks_val],self.labels_val)
      )

def index_to_word(x_train, y_train, index_word_dict,num_data):
  data = []
  for i in range(0,num_data):
    data.append(' '.join(index_word.get(index-3,'') for index in x_train[i]))
  y_train = y_train[:num_data]
  return data,y_train

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(maxlen=512)
imdb_dict = tf.keras.datasets.imdb.get_word_index(path="imdb_word_index.json")
index_word = dict((value,key) for key, value in imdb_dict.items())
x_train_words, train_label = index_to_word(x_train,y_train, index_word,10000) ## test set
x_test_words, test_label = index_to_word(x_test, y_test, index_word,1000)  ## validation set

bert_model = BERT_Classification(x_train_words, train_label, x_test_words, test_label, 2, 12)

fine_tune_model = bert_model.create_model_sequence_output(trainable=True)
bert_model.train(fine_tune_model,2)